
1.1 Careful with aligned and not aligned objects, orientation computation in get_bounding_box_faces might be incorrect


4. find_palm_pose_from_bounding_box_centers might have to be modified in order to ensure good valid samples
5. GraspObjectMessage: For them receives object pose, dimensions, cloud, normals
	5.1 Object pose information currently missing. Is the object pose just the pose of the spawned model or smth else?
6. Make sure the spawned object pose is being published, and make sure it is the right one, which one are they publishing?
7. Extensive testing on preshape sampler
9. Change the segmentation server such that it doesn't store to and load from disk but directly reads the point cloud from the corresponding topic, and then publishes the segmented point cloud
10. Change segmentation server such that it doesn't publish the object information, but rather returns it in response to the service call
11. Make sure panda is being controlled at 1000 Hz


2. Get an intuition for finger closing, i.e. which joints should move how exactly for a good hand closing procedure


1. Change hithand control and joint sampling to smaller range + change hithand control to couple control. Actually the UTAH people don't sample the joint position

1. Get good feeling for different pre shape parameters
	2. Disable all sampling, Find good mean top, find good mean side, find good var
	3. Find good wrist roll sampling range
	4. Think about the 3D noise
2. Integrate the Hithand changes into AR git
3. Test the grasping controller
	j2 0.95, spawn object, test grasping_controller

4.1.21
- Generate 32x32x32 Voxelgrid from PCD as required by Voxel Encoder
- Considerations Tensorflow vs. Pytorch
	- Generally Pytorch is easier to use, faster, build dynamic computation graphs and becoming the default everywhere (PRO Pytorch)
	- However UTAH people, 6DoF GraspNet (Fox, Mousavian) use Python 2.7 and Tensorflow 1.13 (CONTRA Pytorch)
		- PointNet++ is based on Tensorflow
		- There exist Pytorch implementations of PointNet++ but only for versions > Python 3.6. This won't work in melodic.
	- Pytorch and Tensorflow both work in Python 2.7 (Pytorch up to v1.4.0, can be installed via pip
- Starting implementing the voxel-based auto-encoder

5.1.21
- Utah people train autoencoder on reconstruction task and freeze the parameters
	- synthetically render 590 meshes in 200 random positions and backproject (!?!?!) into 3D point cloud
		- Question 1: Do they obtain partial point clouds or full point clouds from this?
		- Question 2: Do they train the AE on reconstruction of partial point clouds to reconstruct partial point clouds? Or to reconstruct full point clouds from partial points clouds, or to reconstruct
			      full points clouds from full points clouds?
		- Question 3: Because weights are frozen should we retrain the conv net?
			- Does the model change when having a different camera setup?
- Regression to a single pose with a neural network is not efficient, due to pose ambiguity (High-dof poses with NN paper does not make sense)
- Learning the grasp success function needs an optimization to find pose
- Idea from Fox/Mousavian to train a VAE to be able to generate multiple grasps and then evaluate for feasibility seems promising
- Training a GAN (InfoGAN) could also be very promising to generate grasp poses

6.1.21
- Good pytorch VAE implementation:
	https://github.com/AndrewSpano/Disentangled_Variational_Autoencoder
- Good thesis on VAE:
	file:///home/vm/Downloads/postgraduate_thesis_variational_autoencoders.pdf
- pip install plyfile
- Utah people use layer_norm instead of batch_norm in grasp_success_net

7.1.21
- needs CUDA 10-1: https://gist.github.com/Mahedi-61/2a2f1579d4271717d421065168ce6a73
	- pip install tensorflow-gpu==1.14
- VoxelAE         [CHECK]
- GraspSuccessNet [CHECK]
- GraspPriorNet   [CHECK]
- What is the right architecture for the ML model?: https://towardsdatascience.com/17-rules-of-thumb-for-building-a-neural-network-93356f9930af#:~:text=The%20first%20layer%20should%20be,is%20the%20number%20of%20classes.
- Using an attention mechanism?
- Deeper insights into how neural networks work, the theory behind it etc.
- grasp representation/ control are important: just controlling hand to a joint position makes no sense
	- Need force feedback, how to incorporate the torque sensing?

8.1.21
- Usage of grasp success labels for pretraining?
- generate grasp object message
- obj_seg server downsamples PCD, maybe change.

9.1.21
- Change object segmentation to return object partial point cloud              [CHECK]
- getting the center from the object cloud makes sense
- getting the size from the oriented bounding box also makes sense, but these values must be transferred to aligned object frame, or not?! [VALIDATE THIS WITH QIAN]
	- Understand what the object centric coordinate system is used for     [CHECK]
	- Generate object centric coordinate system from oriented bounding box [CHECK]
- Change visual data saver to grab point cloud itself                          [CHECK]

10.1.21
- generate voxel and run inference on the model!!!
	- Line 72 grasp_voxel_inference_server
- If that works test/run the whole pipeline
- Reinstall Ubuntu

11.1.21
- grasp voxel inference
- wordings
	suc -- success
	config -- hand_config
	grad -- gradient
	clf -- classifier
	locs_holder -- locs_placeholder
	scales_holder -- scales_placeholder
	logits_holder -- logits_placeholder
	config_holder -- hand_config_placeholder
	tensorflow -- tf
	obj_size -- obj_size_placeholder
	grasp_net -- grasp_success_net
	pred -- predict
	res -- results
12.1.21
- hand config limits are missing

13.1.21
- reinstalling whole machine

14.1.21
- making Gazebo work, finally
- ability to spawn hand without crashing

15.1.21
- WORK
	- spawn hand+panda together
	- test grasp controller
		- actually grasp object
	- simulation runs slower, real time rate only 0.9
		- deactivate unneccessary collision meshes
		- there seems to exist a link to spawning ycb objects
			- FIX Try different object database from github

- [IDEAS]: using grasp type/eigengrasp/grasp primitive to give more structure to the problem and limit search space
	- mark grasp type of all grasps in simulation
	- train a discriminator to decide which grasp type is best suited
	- use average joint state q_bar for this grasp type as starting position for optimization
	- penalize collisions with an extra loss
	- gazebo can only simulate hard contact, soft contact simulation important
	- Because applying force to object when grasping is important, how to find/learn correct force
		- But simulating force and impedance is hard
	- Not only predict where to grasp but how to grasp
	- Using a point cloud as input would enable taking scene context into account
	- BIG IDEA: Grasping in general considered as where to place the fingers, but almost never how to
		    interact with the body
		- Having an impedance controller in simulation would be awesome
		- Most papers try to penetrating the object, thereby often not enabling proper contacts
		- This is kinda what force closure aims at, but in the real world the descrepancies
		   between theoretical joints angles for force closure and actual joint angles that would
		   achieve force closure can be big due to inaccuracies in actuation and sensing

17.1.21
- [IDEA]
	- Different hand representation, instead of just joint angles introduce a finger spread variable and maybe a hand closing variable
	- Design a more elaborate grasping controller!!
	- Sampling multiple grasp poses makes sense, as forward passes are cheap and then performing gradient ascent!
		(https://h2t.anthropomatik.kit.edu/pdf/Ottenhaus2019.pdf)
		- Providing extra information to the voxel input (Pre-trained voxel encoder could not be used anymore)
- [FIX] Make camera model publish slower now at 5 Hz
	- use normal depth/points topic where depth is cutoff at certain distance, not happening for depth image_raw

18.1.21
- [FIX] Try grasping a primitive collision model
	- Works, less downtime
- [TASK] Optimize gazebo
	- [TASK1] Optimize collision models of hand
		- Convex hull makes sense
		-
- [PROBLEM] that the simulation gets super slow when in contact with the object
	- Attempt 1 Simply object and hand geometry
	[FIX] - no camera model +0.1 RTF
	       - Simple hand collision model = +0.1 RTF
	       - Simple object collision model = +-0.0 RTF
19.1.21
- activate conda eval "$(/home/vm/miniconda3/bin/conda shell.bash hook)"
- [Task 1] Add inertia tensors based on Meshlab calculations. [CHECK]
- [Task 2] Find a viable grasp controller setting             [CHECK]
- [Task 3] Automate mesh processing                           [CHECK]
- [Task 4] Optimizing the camera plugin                       [CHECK]
	- Disabling color stream                              [CHECK]
	- Clipping depth readings, clipping point cloud       [CHECK]
- [Task 5] Create grasping demo                               [CHECK]

20.1.21
- [Task 1] Find a viable grasp controller setting            [CHECK]
	- Need joint effort limits of hithand                [CHECK]
	- Need realistic friction coefficients of hithand    [CHECK]
	- Need to think about the grasp closure controller. Right now only controlling one DOF (open close)
		- UTAH people claim to sample preshape and then close the fingers.
	- Need to update collision mesh of hithand palm.     [CHECK]
		(y > -4*(x-0.035)) , z<-0.002
		Palm (y > -4*(x-0.032))
		Middle (y < -4*(x-0.03))
- [Task 2] Talk with Qian about                              [CHECK]
	1. Grasp controller Stopping fingers independently, 0 vel?
Interesting on grasping control, talk about Salisbury and his Impedance grasping controller
#https://www.robotic.de/fileadmin/robotic/borst/BorstEtAl-ICRA03-HandApplications.pdf:
Interesting for grasp impedance adaptation
#https://www.researchgate.net/publication/326774657_A_Self-Tuning_Impedance_Controller_for_Autonomous_Robotic_Manipulation:
#Also promising https://www.cambridge.org/core/journals/robotica/article/abs/development-and-experimental-evaluation-of-multifingered-robot-hand-with-adaptive-impedance-control-for-unknown-environment-grasping/6D2C90B5824543CDCD64A194C5A8A06D:
	2. Grasp representation: use joint angles? Use synergistic control?:
	3. Data collection What should all be recorded? This is the main task, important for whole thesis [CHECK]
		- Collect the data on this machine? Then no development possible in parallel
			- Get other machine or start working on real robot+hand
- [Task 3] Optimize camera model [CHECK]

21.1.21
- Complete tasks still open from last two days
- [Task 1] Couple joints of Hithand in Gazebo
	- Solution 1 just send indentical commands two both joints
- [Task 2] Spawn the fingers in admissible joint range
- PROBLEM Hand would vibrate and do weird stuff
	- Solution Set joint friction to 0 but damping to 10
		- Might be useful to also change controller settings, expecially p
		  gain from 100 to 10 e.g.
	- Update, set realistic joint limits and effort limits + set damping to 0.5

23.1.21
- [IDEA grasping]: Control the finger tip forces, apply virtual springs between fingertip contact and object center. Control the force and increase for individual fingers if you deviate from center.
	- You could first estimate the object center in 3D space
	- Then move the finger until contact is detected
	- Record this position in 3D space
	- Attach springs between finger tip position and and object center. Compute force based on deviation from the center.
		- In the contact position there should already be some kind of force applied. But if the thumb moves away from it's position it would increase the it's force while the other fingertips would decrease force.
	- For this I would need to forward kinematics of the hand, but that should be easily obtained. I don't need to estimate much about the contacts just regulate the finger tip positions to remain in this position

24.1.21
- [Task 1]: Updated the hithand collision meshes
- [Task 2]: Verify once more the correct behavior of grasping controller
- [Task 3]: Make the preshape sampling work + check problems with reachability. Maybe the workspace is too big.
	- For utah people width=x, height=y, depth=z
	- For me 	  width=x, height=z, depth=y
	- Actually this isn't true and just depends on rotation of the bounding box coordinate frame
	- Define center of voxel grid to be the centroid of of points in object segmentation
	- Estimate the first and second principle axes from PCA to create object reference frame BUT SAY THE FRAME IS ALIGNED RELATIVE TO WORLD FRAME!
		- Compute the object size along the three coordinates of the object reference frame
	- They always compute the first and second principal component, which means the width is the longest and height is the second longest dimension. Depth is then the 		  remaining dimension
		- If the oriented bounding box also uses first and second principal axes then width height and depth should be consistent
	- oriented bounding box open3d 1. value = longest = x, 2. value = medium = y, 3. value = low = z

25.1.21
- [Insight]: Utah people set all friction parameters to 1. Of objects and of hand
	- Meshes of the objects potentially should not be very very wrong
	- [Resource]: Gives all the masses for YCB objects https://ri.cmu.edu/pub_files/2015/7/ICAR-FINAL.pdf
- [Task 1]: Test entire grasping pipeline
	- [Finding 1] Somehow moveit considers self-collision of the hand/robot and won't generate a plan
	- [Finding 2] Palm should be towards shorter of two sides                   [CHECK]
	- [Finding 3] Need to find good values for hithand-preshape sampling        [CHECK]
	- [Finding 4] Still need to think about the fact that they say object-centric coordinate system is aligned to world frame  [CHECK]
		- Potentially should use the bounding box center and not centroid of object cloud [CHECK]
- [Task 2]: Optimize performance of grasp pipeline
	- make moveit scene updateer MUCH faster        [CHECK]
	- reset hithand only if needed                  [CHECK]
	- verify joint angles to be correct             [CHECK]
- [Task 3]: Verify influence of visual mesh/material
on performance, UTAH people have this turned off        [CHECK]

26.1.21
- [Task 1] Optimized the whole pipeline for speed       [CHECK]
- [Task 2] Suggestion Karan change the motion planning:
- [Task 3] Integrate everything in the grasp pipeline script
	- [Consid. 1] How many grasps in same position? Top vs. side? Balanced dataset? How many grasps?:
	- [Consid. 2] Does it make sense to spawn object in same position but not save data again/ segment? --> Verify that that's no problem                         [CHECK}

27.1.21
- [Task 1] Integrate everything in the grasp pipeline script
	- generate new depth/ color/ pcd img save paths each time  [CHECK}
	- add reset hithand and panda                              [CHECK]
	- execute panda and hithand reset in parallel              [CHECK]
	- give grasp_type to grasp_type chooser?                   [CHECK]
- [Task 2] Save all grasping data                                  [CHECK]
	- What characterises a grasp
		0.) Metadata
			- Grasp number
			- Grasp type
			- Grasp object
			- Grasp pose
		a.) Used for training the ML model directly
			- 1. hithand config 5*2 = 10 joints + 6D palm pose = 16 values
				- 6D palm pose must be in object frame
			- 2. 32x32x32 voxel grid representation of the object = 32768 values
			- 3. object height x width x depth = 3 values
			- 4. grasp label, success/failure = 1 value
			- 5. grasp stability?!
				- Largest disturbance to counter_act?
				- Or binary, if it can resist certain disturbance
		b.) Visual data on the grasp
			- 1. RGB/depth/scene_pcd
			- 2. object_pcd
			- 3. RGB/depth/pcd? post grasp
	- create group for each grasp
		- create dataset for each value that is important
29.1.21
- [Task 1] Test the data save mechanism                            [CHECK]
- [Task 2] Make entire data acquistion pipeline work
		- what makes up a grasp

31.1.21
- [Task 1] Download Bigbird dataset                                        [CHECK]
- [Task 2] Download KIT dataset                                            [CHECK]
- [Task 3] Create new folder with subfolder for each dataset
	- in each dataset-subfolder should be subfolders for each object   [CHECK]
		- in each object-subfolder should be
			- 1. .sdf file
				- should contain inertial params, also friction
			- 2. visual mesh
			- 3. collision mesh
	- [Finding 1] Apply extra transformation to inertia tensor. Hithand model inertia tensor might be incorrect:
		- trimesh cant work with dae. Maybe convert hithand models to stl/obj and then try inertia transformation (priority is medium to low)

01.2.21-07.02.21
- [Task 1] Finish mesh preparation , prepare_objects_gazebo kit dataset            [CHECK]
	- [Finding 1] KIT need roll of 1.57 to be upright
	- [Finding 2] Material does not look correct for Chips Can
- [Task 2] Finish data acquisition pipeline to be able to do everything            [CHECK]
           from spawning an object to recording data
	- Problem with the grasp and lift API as it needs to choose
		different poses unitl it finds a valid one
	- Reset panda not working                                                  [CHECK]
- [Task 3] Find good camera real pose:
	- D415 works much better than D455
	- x-y-z should be 0.4 - 0.65 - 0.35
- [Task 4] Setup second machine in the office:
- [Task 5] Integrate D415 model into Gazebo, replace D435
- [Task 6] Sample joint angles yes no?
	- If yes randomly or more intelligent?
- [Task 7] Make grasp sampling work                                                [CHECK]
	- find good settings for sample parameters                                 [CHECK]
	- make sure thumb is always pointing up                                    [CHECK]
	- come up with a good way to prune grasps which are not executed           [CHECK]
- [Task 8] Create list with only feasible objects from all datasets                [CHECK]
- [Task 9] Save images as object name                                              [CHECK]
- [Task 10] Fix discrepancies of object centric reference frame
	    and storage of dim_w_h_d
	- For utah people
		(middle-big) width=x,
		(smallest)   height=y,
		(biggest)    depth=z
	- Segmentation working
		1. Transform point cloud to world frame
		2. Find cloud bounding box PCA
			- Centroid, eigenvalues and eigenvectors from PCA
			- find 3rd column of rotation matrix as cross product of the first two
			- transform point cloud from world frame to object centric frame
			- find x,y,z bounds of new transformed point cloud
- [Task 11] Process the collision meshes from text file:
- [Task 12] Integrate the full mesh voxelization                                    [CHECK]
- [Task 13] Find realistic workspace bounds:
	- Possible ranges x - [0.25, 0.65], y - [-0.2, 0.2], :
- [Task 14] Decide which center to use as object cloud center bounding box or cloud
	- decided for point cloud mean
- [Task 15] Treat KIT objects special, spawn high and roll pi/2                     [CHECK]
- [Task 16] Save palm pose in object frame                                          [CHECK]
- [Task 17] Parallel execute panda and hithand reset motion                         [CHECK]
- [Task 18] Prune out poses for which no IK solution can be obtained                [CHECK]
- [Task 19] Top grasp might generate wrong palm orientation                         [CHECK]
- [Task 20] What if no IK solution found for lift position?                         [CHECK]
- [Task 21] Too often no IK solution can be obtained                                [CHECK]
- [Task 22] Test record data save server                                            [CHECK]
- [Task 23] Set Realsense realistic values ALSO FOR IMAGE SIZE                      [CHECK]
- [Task 24] Idea For top grasps --> first move above object then go down
	- Top grasp wrong alignment of thumb
	- Also for side grasps move to a position away from the obejct first, then  [CHECK]
		approach it
		- if the first pose is not reachable, then do it and go straight
- [Task 25] Set moveit workspace boundaries:
- [Task 26] Distinguish between side one and side two grasps                        [CHECK]

DEPENCIES CHANGES: sudo apt-get install ros-melodic-trac-ik-kinematics-plugin

- PAPER https://arxiv.org/pdf/2012.09696.pdf Multi Fin Gan Coarse to Fine

8.2.21
Current big tasks
- [Task 1] Make grasp sampling cover more of the object surface:
- [Task 2] Sample joint angles for hand:
- [Task 3] Distinguish between side one and side two grasps                         [CHECK]
	- Maybe just move further away in normal direction and just increase z
- [Task 4] If no motion plan can be obtained, skip the recording                    [CHECK]
- [Task 5] Generate multiple poses along the object normal
- [Task 6] Sample uniformly instead of Gaussian
- [Task 7] Check why grasps are not stored properly
- [Task 8] Use IK from filter server:
- [Task 9] Stop the pinky if all other fingers stopped
- Problems:
	- Not sampling hithand joint state:
	- Not covering everything of the object surface:
	- Not evaluating stability:
	- Not setting realistic friction parameters:
- IDEA:
	- 1. also store at least some of the poses which do not result in a feasible plan by moveit
	- 2. Use Point-Net++ or similar to output coarse distribution of good grasp, possibly with label 			for grasp type?
		- Refine the initial guess
	- 3. Use pre-trained Point-Net in simulation for data acquisition
		- Input is a partial object point cloud, from this PointNet predicts a set of 			potentially interesting grasp points
		- Around the predicted grasp points sample hypothesis, test these hypothesis in 		simulation and thereby generate ground truth labels (?)
	- 4. Prune out grasps which are already in collision, but not via moveit but by spawning the 			hand in desired position and checking collisions
	- 5. Could give all sampled poses to server to evaluate IK solutions. If no solution found 			already prune out the grasp
	- 6. How to incorporate grasp type?
	- 7. Use surface features like curvature and normal more explicitly?

12.2.21
- Trained Autoencoder works reasonably well:
	- Idea 1 Incorporate object size explicitly into training, to sample more informed
	- Idea 2 Use way more GraspIt data for sampling
- PCA also worked but not as well:
- How to evaluate them?:
- Try human designed features for grasp sampling
- Create a package from grasp-dof-reduction code with a server to sample a full dof joint pos
- Integrate human sample mechanism in generate hithand preshape

- [Task 1]: Implement human eigengrasps            [CHECK]
	- Choose DoF, DoFs that make sense
		1. thumb abduction
		2. Metacarpophalangeal (MCP) joints (where the fingers meet the palm)  (also of thumb)
		3. Proximalinterphalangeal (PIP) joints (joint between upper/lower part of the finger) (also of thumb)
		4. Finger spread, only four fingers
	- Find suitable min max for all degrees of freedom

13.2.21
- Rethink grasp sampling
- [Task 1] Eliminate some more objects which do not have good meshes   [CHECK]
- IDEA:
	- Have one network to predict grasp regions/ grasp points / confidence/ success

14.2.21
- [Task 1] Find max and min for sampling in normal distance
	- max about 12cm (ca. difference from ring phadist to palm link hithand
	- min about 4 cm (ca diff between basecover and palm link hithand
	- 10cm between thumb phadist and middle phadist. Object can not be bigger than 12cm
	- sample max should be 0.2 if object is 12cm should be 1 if object is 2cm
curr_object_size
15.2.21
- [Task 1]
	- Redesign grasp sampling pipeline
	- Redesign grasp data saver
