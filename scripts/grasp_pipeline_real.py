import sys
import os
import time
import copy

import numpy as np
from scipy.spatial.transform import Rotation as R
import matplotlib.pyplot as plt
import math

# Add GraspInference to the path and import
sys.path.append(os.path.join(os.environ['GRASP_PIPELINE_PATH'], 'src'))
from graspInference import *

# DDS imports
import ar_dds as dds

# Create skew-symmetric matrix from vector
def skew(x):
    return np.array([[0, -x[2], x[1]],
                     [x[2], 0, -x[0]],
                     [-x[1], x[0], 0]])

# Create rotation matrix around z-axis
def rot_z(z):
    return np.array([[math.cos(z), -math.sin(z), 0],
                     [math.sin(z), math.cos(z), 0],
                     [0, 0, 1]])

# Load settings
simulation = int(os.environ["SIMULATION"])
use_diana7 = int(os.environ["USE_DIANA7"])

flange_rotation = np.identity(3)
if not use_diana7:
    # Diana X1 flange rotation
    flange_rotation[0,0] = -1
    flange_rotation[2,2] = -1

# Transformations
R_panda = R.from_euler('xyz', [math.pi, 0, 0]).as_matrix() # FFHNet was trained with Franka Panda
hand_R_ee = flange_rotation @ R.from_euler('xyz', [-0.15, -1.3, 0]).as_matrix().T @ R_panda # from URDF
hand_t_ee = np.array([-0.02, 0, 0.06]) # from URDF

cam_R_ee = np.zeros((3, 3))
cam_R_ee[0,1] = -1
cam_R_ee[1,0] = -1
cam_R_ee[2,2] = -1
cam_R_ee =  cam_R_ee @ flange_rotation

cam_t_ee = flange_rotation @ np.array([0.065, 0, 0.01]) # TODO: Proper hand-eye calibration

# DDS subscriber functions
def interrupt_signal_handler(_signal_number, _frame):
    """SIGINT/SIGTSTP handler for gracefully stopping an application."""
    print("Caught interrupt signal. Stop application!")
    global shutdown_requested
    shutdown_requested = True

robot_pos_init = np.zeros(3)

def poll_telemetry_data():
    received_data = listener.take(True)
    for sample in received_data:
        if not sample.info.valid:
            continue

        T = sample.data[dds_telemetry_topic_name]

        pos = T["translation"]
        robot_pos = flange_rotation @ np.array([pos['x'], pos['y'], pos['z']])
        robot_pos -= robot_pos_init

        quat = T["rotation"]
        quat_arr = np.array([quat['x'], quat['y'], quat['z'], quat['w']])
        robot_rot = R.from_quat(quat_arr)
        
        # Translate into EE-frame
        robot_rot = R.from_matrix(robot_rot.as_matrix().T @ flange_rotation)

    return robot_pos, robot_rot

# Initialize DDS Domain Participant
participant = dds.DomainParticipant(domain_id=0)

# Create subscriber using the participant
if use_diana7:
    print("[Info] Using Diana 7.")
    listener = participant.create_subscriber_listener("ar::interfaces::dds::robot::diana::diana7::telemetry_diana_v2",
                                                      "diana7.telemetry_diana_v2", None)
    dds_telemetry_topic_name = "flange_pose"
    print("[Info] Telemetry subscriber is ready. Waiting for data ...")

else:
    print("[Info] Using Diana X1.")
    dds_telemetry_topic_name = "T_flange2base"
    listener = participant.create_subscriber_listener("ar::interfaces::dds::robot::generic::telemetry_v1",
                                                      "diana.telemetry_v1", None)
    print("[Info] Telemetry subscriber is ready. Waiting for data ...")

    # Create Domain Participant and a publisher to publish velocity
    publisher = participant.create_publisher("ar::frankenstein_legacy_interfaces::dds::robot::diana::linear_speed_servoing_v1",
                                            'des_cart_vel_msg')

# Set hyperparameters
n_poses = 1000                      # Number of poses generated by FFHGenerator in every time step
grasp_execution_threshold = 0.85    # Expected success by FFHEvaluator for grasp execution
dist_align_rot = 0.1                # Distance to object in m when also considering rotation of
                                    # grasp (not only align z axis of EE with object direction)
k_p = [1.0, 2.0]                    # Constant control parameters
k_i = [0.0, 0.0]                    # Integrative control parameters
k_d = [0.,0.]#[0.5, 0.5]                    # Derivative control parameters
v_des_max = [0.1, 0.25]              # Limit of linear and angular velocity

# Initialize variables
time_abs = 0.0
x_update = np.zeros(3)
r_update = np.zeros(3)
dx_update_prev = np.zeros(3)
dr_update_prev = np.zeros(3)

# Construct class for FFHNet inference
gi = GraspInference()

# Initialize lists for plot
palm_pos_list = []
palm_rot_list = []
p_max_list = []
p_best_list = []
norm_pos_diff_list = []
norm_rot_diff_list = []
current_success_list = []
#closest_success_list = []
center_pos_list = []
c_list = []
grasp_pos_list = []
grasp_rot_list = []
robot_pos_list = []
robot_rot_list = []

# Get current pose of the robot
robot_pos_init, _ = poll_telemetry_data()

# Warmup
grasp_dict, center_transf = gi.handle_infer_grasp_poses(n_poses)
p_success = gi.handle_evaluate_grasp_poses(grasp_dict)

if use_diana7:
    # Start velocity stream with ar-toolkit
    from ar_toolkit.robots import Diana7Robot
    robot = Diana7Robot("diana7")
    robot.start_linear_speed_listener(0.4, 1) # 50 Hz, flange (0: base, 1:flange, 2:tcp)
    robot.linear_speed_servoing(np.zeros(6))
    print("[Info] Started linear velocity stream.")

rand_arr = np.random.rand(6) * np.pi
i = 0
start = time.process_time()
try:
    while i < 1000000:
        # Run inference on FFHNet
        if int(os.environ['VISUALIZE']):
            grasp_dict, _ = gi.handle_infer_grasp_poses(n_poses)

            # To debug transformations
            #grasp_dict['rot_matrix'] = torch.matmul(grasp_dict['rot_matrix'], torch.tensor(hand_R_ee).float().cuda())
            #grasp_dict['rot_matrix'] = torch.matmul(torch.tensor(cam_R_ee).float().cuda(), grasp_dict['rot_matrix'])
            #grasp_dict['transl'] = torch.tensor(grasp_dict['transl'].detach().cpu().numpy() @ cam_R_ee.T + cam_t_ee).float().cuda()

            grasp_dict_cpu = {}
            for key, item in grasp_dict.items():
                grasp_dict_cpu[key] = item.detach().cpu().numpy()

            visualization.show_generated_grasp_distribution(
                  gi.pcd_path, grasp_dict_cpu)
            # grasp_dict = gi.handle_evaluate_and_filter_grasp_poses(
            #     grasp_dict, thresh=-1)
            exit()

        else:
            grasp_dict, center_transf = gi.handle_infer_grasp_poses(n_poses)
            p_success = gi.handle_evaluate_grasp_poses(grasp_dict)

        if not any(center_transf[0]):
            print("Tracking lost. Stopping robot.")
            break

        # Rotate grasp poses into end effector frame
        grasp_dict_rot = torch.matmul(grasp_dict['rot_matrix'], torch.tensor(hand_R_ee).float().cuda())
        grasp_dict_rot = torch.matmul(torch.tensor(cam_R_ee).float().cuda(), grasp_dict_rot)

        # Save rotation and translation of grasp poses
        grasp_rot = R.from_matrix(grasp_dict_rot.detach().cpu().numpy())
        grasp_pos = grasp_dict['transl'].detach().cpu().numpy() @ cam_R_ee.T + cam_t_ee
        center_translation = (center_transf[:, :3] @ cam_R_ee.T + cam_t_ee)[0]

        # Update rotation and translation
        robot_pos, robot_rot = poll_telemetry_data()
        robot_rot_mat = robot_rot.as_matrix()
        robot_rot = robot_rot.as_rotvec()

        if simulation:
            # Update grasp poses (simulation, in reality point clouds are updated)
            grasp_pos = grasp_pos - robot_pos
            grasp_rot = R.from_matrix(robot_rot_mat.T @ grasp_rot.as_matrix())
            center_translation = center_translation @ robot_rot_mat
            center_translation -= robot_pos
            center_pos_list.append(copy.deepcopy(center_translation))

        # Calculate norm of translational and roational offsets
        diff_pos = np.linalg.norm(grasp_pos, axis=1)
        diff_rot = np.linalg.norm(grasp_rot.as_rotvec(), axis=1)

        # Find best grasp
        p_best = np.argmax(p_success - 0.1*diff_pos - 0.1*diff_rot)

        # Set orientation towards center of point clound (align -z of EE with vector to center)
        # https://math.stackexchange.com/questions/180418/calculate-rotation-matrix-to-align-vector-a-to-vector-b-in-3d
        center_translation /= np.linalg.norm(center_translation)
        v_hat = skew(np.array([center_translation[1], -center_translation[0], 0]))
        rot_align_z_mat = np.identity(3) + v_hat - center_translation[2] * v_hat @ v_hat

        # Rotate around z to to align grasp pose
        rot_align_z = R.from_matrix(rot_z(grasp_rot.as_rotvec()[p_best, 2]) @ rot_align_z_mat).as_rotvec()

        # Metric to banalnce aligning z and grasp rotation
        if use_diana7:
            c_bal = np.clip(-1/dist_align_rot*grasp_pos[p_best, 2], 0.0, 1.0)
        else:
            c_bal = np.clip(1/dist_align_rot*grasp_pos[p_best, 2], 0.0, 1.0)
        rot_des = c_bal * rot_align_z + (1-c_bal) * grasp_rot.as_rotvec()[p_best]
        c_list.append(c_bal.copy())

        # Append results to list (for plot)
        norm_pos_diff_list.append(diff_pos[p_best])
        norm_rot_diff_list.append(diff_rot[p_best])
        palm_pos_list.append(grasp_pos[p_best])
        palm_rot_list.append(rot_des)
        p_best_list.append(p_success[p_best].copy())
        grasp_pos_list.append(grasp_pos[p_best] + robot_pos)
        grasp_rot_list.append(grasp_rot[p_best].as_rotvec() + robot_rot)
        robot_pos_list.append(robot_pos)
        robot_rot_list.append(robot_rot)

        # Get max probability of all grasps
        p_argmax = np.argmax(p_success)
        p_max_list.append(p_success[p_argmax].copy())

        # Move robot towards grasp
        dx_update = grasp_pos[p_best]
        x_update += dx_update
        ddx_update = (dx_update - dx_update_prev)
        dx_update_prev = dx_update
        dr_update = rot_des
        r_update += dr_update
        ddr_update = (dr_update - dr_update_prev)
        dr_update_prev = dr_update
        lin_update = k_p[0] * dx_update + k_i[0] * x_update + k_d[0] * ddx_update
        rot_update = k_p[1] * dr_update + k_i[1] * r_update + k_d[1] * ddr_update

        # Limit linear and angular velocity
        lin_update = np.clip(lin_update, -v_des_max[0], v_des_max[0])
        rot_update = np.clip(rot_update, -v_des_max[1], v_des_max[1])

        # Publish desired velocity
        v_des = np.concatenate([lin_update, rot_update])

        if use_diana7:
            robot.linear_speed_servoing(v_des)
        else:
            publisher.message["dT"] = v_des
            publisher.publish()

        # Check probability of current configuration to succeed
        grasp_current = dict()
        if simulation:
            grasp_current['transl'] = torch.tensor((robot_pos.reshape((1, -1)) - cam_t_ee) @ cam_R_ee).float().cuda()
            grasp_current['rot_matrix'] = torch.matmul(torch.tensor(cam_R_ee.T).float().cuda(), torch.tensor(robot_rot_mat.reshape((1, 3, 3))).float().cuda())
        else:
            grasp_current['transl'] = torch.tensor(-cam_t_ee.reshape((1, -1)) @ cam_R_ee).float().cuda()
            grasp_current['rot_matrix'] = torch.matmul(torch.tensor(cam_R_ee.T).float().cuda(), torch.eye(3)[None].cuda())
        grasp_current['rot_matrix'] = torch.matmul(grasp_current['rot_matrix'], torch.tensor(hand_R_ee.T).float().cuda())
        grasp_current['joint_conf'] = grasp_dict['joint_conf'][p_best]
        grasp_current['z'] = grasp_dict['z'][p_best]
        grasp_current['bps_object'] = grasp_dict['bps_object'][p_best]
        current_success = gi.handle_evaluate_grasp_poses(grasp_current)
        print("Current success: ", current_success)
        current_success_list.append(current_success.copy())

        # # Find fitting grasp
        # print(grasp_pos)
        # print(grasp_rot.as_rotvec())
        # print(np.stack(grasp_pos, grasp_rot.as_rotvec()))
        # grasp_closest = np.argmin(np.linalg.norm(np.concatenate(grasp_pos, grasp_rot.as_rotvec()) - np.concatenate(robot_pos, robot_rot), axis=1))

        # Check probability of current configuration to succeed
        # grasp_current = dict()
        # if simulation:
        #     grasp_current['transl'] = torch.tensor((robot_pos.reshape((1, -1)) - cam_t_ee) @ cam_R_ee).float().cuda()
        #     grasp_current['rot_matrix'] = torch.matmul(torch.tensor(cam_R_ee.T).float().cuda(), torch.tensor(robot_rot_mat.reshape((1, 3, 3))).float().cuda())
        # else:
        #     grasp_current['transl'] = torch.tensor(-cam_t_ee.reshape((1, -1)) @ cam_R_ee).float().cuda()
        #     grasp_current['rot_matrix'] = torch.matmul(torch.tensor(cam_R_ee.T).float().cuda(), torch.eye(3)[None].cuda())
        # grasp_current['rot_matrix'] = torch.matmul(grasp_current['rot_matrix'], torch.tensor(hand_R_ee.T).float().cuda())
        # grasp_current['joint_conf'] = grasp_dict['joint_conf'][grasp_closest]
        # grasp_current['z'] = grasp_dict['z'][grasp_closest]
        # grasp_current['bps_object'] = grasp_dict['bps_object'][grasp_closest]
        # current_success = gi.handle_evaluate_grasp_poses(grasp_current)
        # current_success_list.append(current_success.copy())

        # Time analysis
        i += 1
        time_loop = time.process_time() - start - time_abs
        print("Time of last cycle: ", time_loop)
        time_abs += time_loop

        # Execute grasp if probability is higher than threshold
        if current_success > grasp_execution_threshold:
            print("\n####################")
            print("# Grasp Execution! #")
            print("####################\n")
            break

except KeyboardInterrupt:
    pass

print("Overall time: ", time_abs)
print("Avg time per cicle: ", time_abs/i, " | Cicles per second: ", i/time_abs)

# Stop motion
if use_diana7:
    robot.linear_speed_servoing(np.zeros(6))
else:
    publisher.message["dT"] = np.zeros(6)
    publisher.publish()

# # Plot grasp distribution
# grasp_dict_cpu = {}
# for key, item in grasp_dict.items():
#     if key == 'bps_object':
#         continue 
#     grasp_dict_cpu[key] = item.detach().cpu().numpy()
# visualization.show_generated_grasp_distribution(
#                   gi.pcd_path, grasp_dict_cpu)

# Plot result
#plt.plot(palm_pos_list, label=["x", "y", "z"])
#plt.plot(palm_rot_list, label=["rx", "ry", "rz"])
#plt.plot(p_max_list, label="p_max")
plt.plot(p_best_list, label="p_best")
plt.plot(norm_pos_diff_list, label="delta_x")
plt.plot(norm_rot_diff_list, label="delta_rot")
seq_len = len(current_success_list)
plt.plot(grasp_execution_threshold*np.ones(seq_len), label="grasp_execution_threshold")
plt.plot(current_success_list, label="exp_success")
#plt.plot(closest_success_list, label="exp_success_clostest_grasp")
# plt.plot(center_pos_list, label="center_pos")
plt.plot(c_list, label="c_bal")
#plt.plot(grasp_pos_list, label=["grasp_x", "grasp_y", "grasp_z"])
#plt.plot(grasp_rot_list, label=["grasp_rx", "grasp_ry", "grasp_rz"])
#plt.plot(robot_pos_list, label=["robot_x", "robot_y", "robot_z"])
#plt.plot(robot_rot_list, label=["robot_rx", "robot_ry", "robot_rz"])
plt.legend(loc="upper left")
plt.show()