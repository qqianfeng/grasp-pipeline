
1.1 Careful with aligned and not aligned objects, orientation computation in get_bounding_box_faces might be incorrect


4. find_palm_pose_from_bounding_box_centers might have to be modified in order to ensure good valid samples
5. GraspObjectMessage: For them receives object pose, dimensions, cloud, normals
	5.1 Object pose information currently missing. Is the object pose just the pose of the spawned model or smth else?
6. Make sure the spawned object pose is being published, and make sure it is the right one, which one are they publishing?
7. Extensive testing on preshape sampler
9. Change the segmentation server such that it doesn't store to and load from disk but directly reads the point cloud from the corresponding topic, and then publishes the segmented point cloud
10. Change segmentation server such that it doesn't publish the object information, but rather returns it in response to the service call
11. Make sure panda is being controlled at 1000 Hz


2. Get an intuition for finger closing, i.e. which joints should move how exactly for a good hand closing procedure


1. Change hithand control and joint sampling to smaller range + change hithand control to couple control. Actually the UTAH people don't sample the joint position

1. Get good feeling for different pre shape parameters
	2. Disable all sampling, Find good mean top, find good mean side, find good var
	3. Find good wrist roll sampling range
	4. Think about the 3D noise
2. Integrate the Hithand changes into AR git
3. Test the grasping controller
	j2 0.95, spawn object, test grasping_controller

4.1.21
- Generate 32x32x32 Voxelgrid from PCD as required by Voxel Encoder
- Considerations Tensorflow vs. Pytorch
	- Generally Pytorch is easier to use, faster, build dynamic computation graphs and becoming the default everywhere (PRO Pytorch)
	- However UTAH people, 6DoF GraspNet (Fox, Mousavian) use Python 2.7 and Tensorflow 1.13 (CONTRA Pytorch)
		- PointNet++ is based on Tensorflow
		- There exist Pytorch implementations of PointNet++ but only for versions > Python 3.6. This won't work in melodic. 
	- Pytorch and Tensorflow both work in Python 2.7 (Pytorch up to v1.4.0, can be installed via pip 
- Starting implementing the voxel-based auto-encoder

5.1.21
- Utah people train autoencoder on reconstruction task and freeze the parameters
	- synthetically render 590 meshes in 200 random positions and backproject (!?!?!) into 3D point cloud 
		- Question 1: Do they obtain partial point clouds or full point clouds from this?
		- Question 2: Do they train the AE on reconstruction of partial point clouds to reconstruct partial point clouds? Or to reconstruct full point clouds from partial points clouds, or to reconstruct 
			      full points clouds from full points clouds?
		- Question 3: Because weights are frozen should we retrain the conv net?
			- Does the model change when having a different camera setup?
- Regression to a single pose with a neural network is not efficient, due to pose ambiguity (High-dof poses with NN paper does not make sense)
- Learning the grasp success function needs an optimization to find pose
- Idea from Fox/Mousavian to train a VAE to be able to generate multiple grasps and then evaluate for feasibility seems promising
- Training a GAN (InfoGAN) could also be very promising to generate grasp poses

6.1.21
- Good pytorch VAE implementation:
	https://github.com/AndrewSpano/Disentangled_Variational_Autoencoder
- Good thesis on VAE:
	file:///home/vm/Downloads/postgraduate_thesis_variational_autoencoders.pdf
- pip install plyfile
- Utah people use layer_norm instead of batch_norm in grasp_success_net

7.1.21
- needs CUDA 10-1: https://gist.github.com/Mahedi-61/2a2f1579d4271717d421065168ce6a73
	- pip install tensorflow-gpu==1.14
- VoxelAE         [CHECK]
- GraspSuccessNet [CHECK]
- GraspPriorNet   [CHECK]
- What is the right architecture for the ML model?: https://towardsdatascience.com/17-rules-of-thumb-for-building-a-neural-network-93356f9930af#:~:text=The%20first%20layer%20should%20be,is%20the%20number%20of%20classes.
- Using an attention mechanism?
- Deeper insights into how neural networks work, the theory behind it etc.
- grasp representation/ control are important: just controlling hand to a joint position makes no sense
	- Need force feedback, how to incorporate the torque sensing?

8.1.21
- Usage of grasp success labels for pretraining?
- generate grasp object message
- obj_seg server downsamples PCD, maybe change.

9.1.21
- Change object segmentation to return object partial point cloud              [CHECK]
- getting the center from the object cloud makes sense
- getting the size from the oriented bounding box also makes sense, but these values must be transferred to aligned object frame, or not?! [VALIDATE THIS WITH QIAN]
	- Understand what the object centric coordinate system is used for     [CHECK]
	- Generate object centric coordinate system from oriented bounding box [CHECK]
- Change visual data saver to grab point cloud itself                          [CHECK]

10.1.21
- generate voxel and run inference on the model!!! 
	- Line 72 grasp_voxel_inference_server
- If that works test/run the whole pipeline
- Reinstall Ubuntu

11.1.21 
- grasp voxel inference
- wordings:
	suc -- success
	config -- hand_config
	grad -- gradient
	clf -- classifier
	locs_holder -- locs_placeholder
	scales_holder -- scales_placeholder
	logits_holder -- logits_placeholder
	config_holder -- hand_config_placeholder
	tensorflow -- tf
	obj_size -- obj_size_placeholder
	grasp_net -- grasp_success_net
	pred -- predict
	res -- results
12.1.21
- hand config limits are missing

13.1.21
- reinstalling whole machine

14.1.21
- making Gazebo work, finally
- ability to spawn hand without crashing

15.1.21
- WORK:
	- spawn hand+panda together
	- test grasp controller
		- actually grasp object
	- simulation runs slower, real time rate only 0.9
		- deactivate unneccessary collision meshes
		- there seems to exist a link to spawning ycb objects
			- FIX: Try different object database from github	

- [IDEAS]: using grasp type/eigengrasp/grasp primitive to give more structure to the problem and limit search space
	- mark grasp type of all grasps in simulation
	- train a discriminator to decide which grasp type is best suited
	- use average joint state q_bar for this grasp type as starting position for optimization
	- penalize collisions with an extra loss
	- gazebo can only simulate hard contact, soft contact simulation important
	- Because applying force to object when grasping is important, how to find/learn correct force
		- But simulating force and impedance is hard
	- Not only predict where to grasp but how to grasp
	- Using a point cloud as input would enable taking scene context into account
	- BIG IDEA: Grasping in general considered as where to place the fingers, but almost never how to 
		    interact with the body
		- Having an impedance controller in simulation would be awesome
		- Most papers try to penetrating the object, thereby often not enabling proper contacts
		- This is kinda what force closure aims at, but in the real world the descrepancies
		   between theoretical joints angles for force closure and actual joint angles that would
		   achieve force closure can be big due to inaccuracies in actuation and sensing

17.1.21
- [IDEA]
	- Different hand representation, instead of just joint angles introduce a finger spread variable and maybe a hand closing variable
	- Design a more elaborate grasping controller!!
	- Sampling multiple grasp poses makes sense, as forward passes are cheap and then performing gradient ascent!
		(https://h2t.anthropomatik.kit.edu/pdf/Ottenhaus2019.pdf)
		- Providing extra information to the voxel input (Pre-trained voxel encoder could not be used anymore)
- [FIX] Make camera model publish slower now at 5 Hz
	- use normal depth/points topic where depth is cutoff at certain distance, not happening for depth image_raw 

18.1.21 
- [FIX] Try grasping a primitive collision model
	- Works, less downtime
- [TASK] Optimize gazebo
	- [TASK1] Optimize collision models of hand
		- Convex hull makes sense
		-
- [PROBLEM] that the simulation gets super slow when in contact with the object
	- Attempt 1 Simply object and hand geometry 
	[FIX] - no camera model +0.1 RTF
	       - Simple hand collision model = +0.1 RTF
	       - Simple object collision model = +-0.0 RTF
19.1.21
- activate conda eval "$(/home/vm/miniconda3/bin/conda shell.bash hook)" 
- [Task 1] Add inertia tensors based on Meshlab calculations. [CHECK]
- [Task 2] Find a viable grasp controller setting             [CHECK]
- [Task 3] Automate mesh processing                       [Sort of check]:
- [Task 4] Optimizing the camera plugin                       [CHECK]
	- Disabling color stream                              [CHECK]
	- Clipping depth readings, clipping point cloud       [CHECK]
- [Task 5] Create grasping demo                               [CHECK]

20.1.21
- [Task 1] Find a viable grasp controller setting            [CHECK]
	- Need joint effort limits of hithand                [CHECK]
	- Need realistic friction coefficients of hithand    [CHECK]
	- Need to think about the grasp closure controller. Right now only controlling one DOF (open close)
		- UTAH people claim to sample preshape and then close the fingers.
	- Need to update collision mesh of hithand palm.     [CHECK]  
		(y > -4*(x-0.035)) , z<-0.002
		Palm (y > -4*(x-0.032))
		Middle (y < -4*(x-0.03))
- [Task 2] Talk with Qian about                              [CHECK]
	1. Grasp controller Stopping fingers independently, 0 vel? 
Interesting on grasping control, talk about Salisbury and his Impedance grasping controller
#https://www.robotic.de/fileadmin/robotic/borst/BorstEtAl-ICRA03-HandApplications.pdf: 
Interesting for grasp impedance adaptation 
#https://www.researchgate.net/publication/326774657_A_Self-Tuning_Impedance_Controller_for_Autonomous_Robotic_Manipulation:
#Also promising https://www.cambridge.org/core/journals/robotica/article/abs/development-and-experimental-evaluation-of-multifingered-robot-hand-with-adaptive-impedance-control-for-unknown-environment-grasping/6D2C90B5824543CDCD64A194C5A8A06D:
	2. Grasp representation: use joint angles? Use synergistic control?:
	3. Data collection What should all be recorded? This is the main task, important for whole thesis:
		- Collect the data on this machine? Then no development possible in parallel
			- Get other machine or start working on real robot+hand 
- [Task 3] Optimize camera model [CHECK]

21.1.21 
- Complete tasks still open from last two days
- [Task 1] Couple joints of Hithand in Gazebo
	- Solution 1 just send indentical commands two both joints
- [Task 2] Spawn the fingers in admissible joint range
- PROBLEM Hand would vibrate and do weird stuff
	- Solution Set joint friction to 0 but damping to 10
		- Might be useful to also change controller settings, expecially p
		  gain from 100 to 10 e.g.
	- Update, set realistic joint limits and effort limits + set damping to 0.5

23.1.21
- [IDEA grasping]: Control the finger tip forces, apply virtual springs between fingertip contact and object center. Control the force and increase for individual fingers if you deviate from center.
	- You could first estimate the object center in 3D space
	- Then move the finger until contact is detected
	- Record this position in 3D space
	- Attach springs between finger tip position and and object center. Compute force based on deviation from the center.
		- In the contact position there should already be some kind of force applied. But if the thumb moves away from it's position it would increase the it's force while the other fingertips would decrease force.
	- For this I would need to forward kinematics of the hand, but that should be easily obtained. I don't need to estimate much about the contacts just regulate the finger tip positions to remain in this position

24.1.21
- [Task 1]: Updated the hithand collision meshes
- [Task 2]: Verify once more the correct behavior of grasping controller
- [Task 3]: Make the preshape sampling work + check problems with reachability. Maybe the workspace is too big. 
	- For utah people width=x, height=y, depth=z
	- For me 	  width=x, height=z, depth=y
	- Actually this isn't true and just depends on rotation of the bounding box coordinate frame
	- Define center of voxel grid to be the centroid of of points in object segmentation
	- Estimate the first and second principle axes from PCA to create object reference frame BUT SAY THE FRAME IS ALIGNED RELATIVE TO WORLD FRAME!
		- Compute the object size along the three coordinates of the object reference frame
	- They always compute the first and second principal component, which means the width is the longest and height is the second longest dimension. Depth is then the 		  remaining dimension
		- If the oriented bounding box also uses first and second principal axes then width height and depth should be consistent
	- oriented bounding box open3d 1. value = longest = x, 2. value = medium = y, 3. value = low = z

25.1.21
- [Insight]: Utah people set all friction parameters to 1. Of objects and of hand
	- Meshes of the objects potentially should not be very very wrong
	- [Resource]: Gives all the masses for YCB objects https://ri.cmu.edu/pub_files/2015/7/ICAR-FINAL.pdf
- [Task 1]: Test entire grasping pipeline
	- [Finding 1] Somehow moveit considers self-collision of the hand/robot and won't generate a plan
	- [Finding 2] Palm should be towards shorter of two sides:
	- [Finding 3] Need to find good values for hithand-preshape sampling:
	- [Finding 4] Still need to think about the fact that they say object-centric coordinate system is aligned to world frame:
		- Potentially should use the bounding box center and not centroid of object cloud :
- [Task 2]: Optimize performance of grasp pipeline
	- make moveit scene updateer MUCH faster        [CHECK] 
	- reset hithand only if needed                  [CHECK]
	- verify joint angles to be correct             [CHECK]
- [Task 3]: Verify influence of visual mesh/material on performance, UTAH people have this turned off: 

26.1.21 
- [Task 1] Optimized the whole pipeline for speed       [CHECK]
- [Task 2] Suggestion Karan change the motion planning: 
- [Task 3] Integrate everything in the grasp pipeline script
	- [Consid. 1] How many grasps in same position? Top vs. side? Balanced dataset? How many grasps?:
	- [Consid. 2] Does it make sense to spawn object in same position but not save data again/ segment? --> Verify that that's no problem:

27.1.21
- [Task 1] Integrate everything in the grasp pipeline script
	- generate new depth/ color/ pcd img save paths each time:
	- add reset hithand and panda                              [CHECK]
	- execute panda and hithand reset in parallel:
	- give grasp_type to grasp_type chooser?:
- [Task 2] Save all grasping data
	- What characterises a grasp
		0.) Metadata
			- Grasp number
			- Grasp type
			- Grasp object
			- Grasp pose
		a.) Used for training the ML model directly
			- 1. hithand config 5*2 = 10 joints + 6D palm pose = 16 values
				- 6D palm pose must be in object frame
			- 2. 32x32x32 voxel grid representation of the object = 32768 values
			- 3. object height x width x depth = 3 values
			- 4. grasp label, success/failure = 1 value
			- 5. grasp stability?! 
				- Largest disturbance to counter_act?
				- Or binary, if it can resist certain disturbance
		b.) Visual data on the grasp
			- 1. RGB/depth/scene_pcd
			- 2. object_pcd
			- 3. RGB/depth/pcd? post grasp
	- create group for each grasp
		- create dataset for each value that is important
29.1.21
- [Task 1] Test the data save mechanism
